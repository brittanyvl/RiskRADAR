# RiskRADAR Embedding Model Benchmark Report

**Generated:** 2026-01-13 23:12
**Benchmark Version:** 2.0

---

## Executive Summary

| Model | Mean MRR | Hit@10 | nDCG@10 | Latency (p95) |
|-------|----------|--------|---------|---------------|
| MiniLM | 0.669 | 94.9% | 0.570 | 121ms |
| MIKA | 0.788 | 94.9% | 0.643 | 170ms |

**Recommendation:** MIKA (MRR difference: +0.119)

---

## Methodology

### Query Design

| Category | Count | Difficulty Mix | Purpose |
|----------|-------|----------------|---------|
| Incident Lookup | 10 | Easy | Known accidents with specific report IDs |
| Conceptual Queries | 12 | Medium-Hard | Technical concepts requiring semantic understanding |
| Section Queries | 10 | Medium | Queries targeting specific report sections |
| Comparative Queries | 8 | Hard | Analytical queries about patterns |
| Aircraft Queries | 6 | Medium | Aircraft-type specific searches |
| Phase Queries | 4 | Medium | Flight phase specific searches |

### Ground Truth Verification

All ground truth was established via SQL queries against `chunks.parquet`:
- **Incident queries:** Report IDs verified from NTSB report metadata
- **Conceptual queries:** Term co-occurrence verified via LIKE patterns
- **Section queries:** Section names verified from chunk metadata

### Metrics

| Metric | Description |
|--------|-------------|
| MRR | Mean Reciprocal Rank - position of first relevant result |
| Hit@K | Percentage with at least one relevant in top K |
| Precision@K | Fraction of top K that are relevant |
| Recall@K | Fraction of relevant found in top K |
| nDCG@K | Normalized Discounted Cumulative Gain |
| Section Accuracy | Fraction from expected sections (section queries) |

---

## Detailed Results

### MiniLM

**Performance by Category:**

| Category | MRR | Hit@10 | nDCG@10 | Latency |
|----------|-----|--------|---------|---------|
| incident_lookup | 0.750 | 100.0% | 0.814 | 140ms |
| conceptual_queries | 0.589 | 100.0% | 0.375 | 104ms |
| section_queries | *N/A* | *N/A* | *N/A* | 100ms |
| comparative_queries | 0.589 | 87.5% | 0.485 | 107ms |
| aircraft_queries | 0.844 | 83.3% | 0.729 | 106ms |
| phase_queries | 0.625 | 100.0% | 0.532 | 108ms |

*Section queries evaluated on Section Accuracy: **76.0%***

**Performance by Difficulty:**

| Difficulty | MRR | Hit@10 | Latency |
|------------|-----|--------|---------|
| easy | 0.750 | 100.0% | 145ms |
| medium | 0.662 | 93.8% | 104ms |
| hard | 0.625 | 92.9% | 105ms |

### MIKA

**Performance by Category:**

| Category | MRR | Hit@10 | nDCG@10 | Latency |
|----------|-----|--------|---------|---------|
| incident_lookup | 0.889 | 100.0% | 0.918 | 179ms |
| conceptual_queries | 0.764 | 100.0% | 0.420 | 143ms |
| section_queries | *N/A* | *N/A* | *N/A* | 144ms |
| comparative_queries | 0.688 | 75.0% | 0.480 | 148ms |
| aircraft_queries | 0.806 | 100.0% | 0.823 | 134ms |
| phase_queries | 0.812 | 100.0% | 0.750 | 143ms |

*Section queries evaluated on Section Accuracy: **71.0%***

**Performance by Difficulty:**

| Difficulty | MRR | Hit@10 | Latency |
|------------|-----|--------|---------|
| easy | 0.889 | 100.0% | 184ms |
| medium | 0.776 | 100.0% | 141ms |
| hard | 0.738 | 85.7% | 146ms |

---

## Statistical Analysis

### Bootstrap Confidence Interval (Primary)

| Metric | Value |
|--------|-------|
| MRR Difference (MIKA - MiniLM) | +0.093 |
| 95% Confidence Interval | [-0.004, 0.190] |
| Statistically Significant | No |

**Interpretation:** No significant difference detected (CI includes 0).

### Win/Loss/Tie Analysis

| Model | Wins | Percentage |
|-------|------|------------|
| MIKA | 14 | 28.0% |
| MiniLM | 5 | 10.0% |
| Tie | 31 | 62.0% |

*Note: A query is a 'tie' if MRR difference < 0.01*

---

## Streamlit Visualization

Results are saved in Parquet format for easy loading in Streamlit:

```python
import pandas as pd

# Load benchmark results
minilm_df = pd.read_parquet('eval/results/benchmark_minilm_*.parquet')
mika_df = pd.read_parquet('eval/results/benchmark_mika_*.parquet')

# Merge for comparison
comparison = minilm_df.merge(
    mika_df, on='query_id', suffixes=('_minilm', '_mika')
)
```

---

*Report generated by `eval/benchmark.py`*