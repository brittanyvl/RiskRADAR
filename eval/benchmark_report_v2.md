# RiskRADAR Embedding Model Benchmark Report

**Generated:** 2026-01-19 12:13
**Benchmark Version:** 2.0

---

## Executive Summary

| Model | Mean MRR | Hit@10 | nDCG@10 | Latency (p95) |
|-------|----------|--------|---------|---------------|
| MiniLM | 0.704 | 100.0% | 0.625 | 394ms |
| MIKA | 0.816 | 100.0% | 0.675 | 521ms |

**Recommendation:** MIKA (MRR difference: +0.111)

---

## Methodology

### Query Design

| Category | Count | Difficulty Mix | Purpose |
|----------|-------|----------------|---------|
| Incident Lookup | 10 | Easy | Known accidents with specific report IDs |
| Conceptual Queries | 12 | Medium-Hard | Technical concepts requiring semantic understanding |
| Section Queries | 10 | Medium | Queries targeting specific report sections |
| Comparative Queries | 8 | Hard | Analytical queries about patterns |
| Aircraft Queries | 6 | Medium | Aircraft-type specific searches |
| Phase Queries | 4 | Medium | Flight phase specific searches |

### Ground Truth Verification

All ground truth was established via SQL queries against `chunks.parquet`:
- **Incident queries:** Report IDs verified from NTSB report metadata
- **Conceptual queries:** Term co-occurrence verified via LIKE patterns
- **Section queries:** Section names verified from chunk metadata

### Metrics

| Metric | Description |
|--------|-------------|
| MRR | Mean Reciprocal Rank - position of first relevant result |
| Hit@K | Percentage with at least one relevant in top K |
| Precision@K | Fraction of top K that are relevant |
| Recall@K | Fraction of relevant found in top K |
| nDCG@K | Normalized Discounted Cumulative Gain |
| Section Accuracy | Fraction from expected sections (section queries) |

---

## Detailed Results

### MiniLM

**Performance by Category:**

| Category | MRR | Hit@10 | nDCG@10 | Latency |
|----------|-----|--------|---------|---------|
| incident_lookup | 0.911 | 100.0% | 0.932 | 228ms |
| conceptual_queries | 0.610 | 100.0% | 0.431 | 156ms |
| section_queries | *N/A* | *N/A* | *N/A* | 111ms |
| comparative_queries | 0.604 | 100.0% | 0.532 | 116ms |
| aircraft_queries | 0.778 | 100.0% | 0.746 | 104ms |
| phase_queries | 0.613 | 100.0% | 0.518 | 226ms |

*Section queries evaluated on Section Accuracy: **62.0%***

**Performance by Difficulty:**

| Difficulty | MRR | Hit@10 | Latency |
|------------|-----|--------|---------|
| easy | 0.911 | 100.0% | 240ms |
| medium | 0.619 | 100.0% | 135ms |
| hard | 0.669 | 100.0% | 137ms |

### MIKA

**Performance by Category:**

| Category | MRR | Hit@10 | nDCG@10 | Latency |
|----------|-----|--------|---------|---------|
| incident_lookup | 1.000 | 100.0% | 1.000 | 232ms |
| conceptual_queries | 0.623 | 100.0% | 0.422 | 197ms |
| section_queries | *N/A* | *N/A* | *N/A* | 183ms |
| comparative_queries | 0.750 | 100.0% | 0.562 | 264ms |
| aircraft_queries | 1.000 | 100.0% | 0.808 | 143ms |
| phase_queries | 0.833 | 100.0% | 0.727 | 148ms |

*Section queries evaluated on Section Accuracy: **55.0%***

**Performance by Difficulty:**

| Difficulty | MRR | Hit@10 | Latency |
|------------|-----|--------|---------|
| easy | 1.000 | 100.0% | 236ms |
| medium | 0.854 | 100.0% | 176ms |
| hard | 0.653 | 100.0% | 228ms |

---

## Statistical Analysis

### Bootstrap Confidence Interval (Primary)

| Metric | Value |
|--------|-------|
| MRR Difference (MIKA - MiniLM) | +0.087 |
| 95% Confidence Interval | [-0.015, 0.185] |
| Statistically Significant | No |

**Interpretation:** No significant difference detected (CI includes 0).

### Win/Loss/Tie Analysis

| Model | Wins | Percentage |
|-------|------|------------|
| MIKA | 15 | 30.0% |
| MiniLM | 5 | 10.0% |
| Tie | 30 | 60.0% |

*Note: A query is a 'tie' if MRR difference < 0.01*

---

## Streamlit Visualization

Results are saved in Parquet format for easy loading in Streamlit:

```python
import pandas as pd

# Load benchmark results
minilm_df = pd.read_parquet('eval/results/benchmark_minilm_*.parquet')
mika_df = pd.read_parquet('eval/results/benchmark_mika_*.parquet')

# Merge for comparison
comparison = minilm_df.merge(
    mika_df, on='query_id', suffixes=('_minilm', '_mika')
)
```

---

*Report generated by `eval/benchmark.py`*