# RiskRADAR Final Benchmark Report

**Generated:** 2026-01-18 22:00

This report combines automated benchmark metrics with human evaluation of semantic search quality.

---

## Executive Summary

**Recommendation: MIKA - Better automated metrics**

---

## Automated Metrics (30 queries)

Categories: Incident Lookup, Section Queries, Aircraft Queries, Phase Queries

| Model | Mean MRR | Hit@10 |
|-------|----------|--------|
| MiniLM | 0.669 | 94.9% |
| MIKA | 0.788 | 94.9% |

**Difference (MIKA - MiniLM):** +0.119 MRR

---

## Human-Evaluated Metrics (20 queries)

Categories: Conceptual Queries, Comparative Queries

| Model | Semantic Precision | Semantic Lift | False Positive Rate |
|-------|-------------------|---------------|---------------------|
| MiniLM | 75.5% | 13.6% | 16.4% |
| MIKA | 60.0% | 13.6% | 34.5% |

**Key Metrics Explained:**
- **Semantic Precision:** Fraction of results that are relevant (keyword + semantic matches)
- **Semantic Lift:** Fraction of results that are relevant but wouldn't match keywords (the VALUE of semantic search)
- **False Positive Rate:** Fraction of results that are not relevant

**Semantic Lift Difference (MIKA - MiniLM):** +0.0%

---

## Interpretation

MIKA shows better automated metrics but similar semantic lift to MiniLM.
The domain-specific training helps with keyword-matchable content but doesn't
provide significant advantage for pure semantic understanding.

---

*Report generated by `eval/benchmark.py final-report`*